{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting PyTorch Models to ONNX and Running Inference\n",
    "\n",
    "### Overview\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is an open-source format designed for the efficient exchange of machine learning models across different frameworks. It can be seen as a standardized language for neural networks, enabling models trained in one framework (such as PyTorch) to be used in another environment, simplifying deployment and optimizing performance. For more details, refer to the [ONNX documentation](https://onnx.ai/onnx/intro/concepts.html#onnx-concepts).\n",
    "\n",
    "By converting models to the ONNX format, machine learning practitioners can avoid framework-specific dependencies and enable seamless deployment across a wide variety of platforms and languages like C++, Python, Java, or even WebAssembly.\n",
    "\n",
    "### Advantages of Using ONNX\n",
    "\n",
    "- **Framework Interoperability**: ONNX provides a unified representation of models that can be shared and deployed irrespective of the development environment.\n",
    "- **Optimized Inference**: Many runtimes are optimized for ONNX models, which can lead to faster inference times compared to native model formats.\n",
    "- **Flexible Deployment**: ONNX models can be executed in production environments using ONNX-compatible runtimes, making it easier to integrate with applications across different platforms and languages.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Once converted, an ONNX model defines a computational graph consisting of various ONNX operators. During inference, the production environment requires an ONNX-compatible runtime (such as ONNX Runtime) to interpret and execute this graph. Serialization is used to optimize the model's size by compressing the entire model.\n",
    "\n",
    "### PyTorch to ONNX Conversion and Inference\n",
    "\n",
    "In this tutorial, we'll walk through the steps of exporting a PyTorch model to ONNX format and demonstrate how to perform inference using the ONNX Runtime in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for a convnext-tiny model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the model architecture used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNext_tiny_pretrained(nn.Module): # simple implementation of the ConvNext_tiny\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNext_tiny_pretrained, self).__init__()\n",
    "        self.model = models.convnext.convnext_tiny(weights='DEFAULT') # importing the model with pretrained weights\n",
    "        self.model.classifier[-1] = nn.Linear(768, num_classes) # changing the last layer to output num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) # forward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the pre-trained model from path and load the weights to the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'path/to/your/model.pth' # path to the model\n",
    "model = ConvNext_tiny_pretrained() # creating the model\n",
    "model.to(device) # moving the model to the device\n",
    "model.load_state_dict(torch.load(model_path)) # loading the model weights\n",
    "model.eval() # setting the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. set the input size used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (3, 224, 224) # input size of the model\n",
    "input_tensor = torch.rand(1, *input_size) # creating a random input tensor\n",
    "input_tensor = input_tensor.to(device) # moving the input tensor to the device (same as the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, input_tensor, 'ConvNext_tiny_pretrained.onnx', verbose=True) # exporting the model to ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx, onnxruntime\n",
    "import onnx.numpy_helper\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx model path\n",
    "model_onnx_path = 'ConvNext_tiny_pretrained.onnx'\n",
    "# runtime initialization\n",
    "ort_session = onnxruntime.InferenceSession(model_onnx_path, ['CPUExecutionProvider']) # use 'CUDAExecutionProvider' for CPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preparing the images for inference:\n",
    "* Model accepts 3x244x244 RGB images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(path): # path to folder containing images\n",
    "    images = []\n",
    "    for image_path in glob.glob(path + '/*.jpg'): # read all jpg images in the folder\n",
    "        image = cv2.imread(image_path) # read the image\n",
    "        image = cv2.resize(image, (224, 224)) # resize the image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert the image to RGB , because opencv reads the image in BGR format\n",
    "        image = np.transpose(image, (2, 0, 1)) # change the image format to CxHxW (channels first)\n",
    "        image = np.expand_dims(image, axis=0) # add a batch dimension to the image (1, C, H, W), i.e one image per batch\n",
    "        images.append(image) # append the image to the list\n",
    "    return np.array(images) # return the list of images as a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. add the softmax function as it's not inheritely in torch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. function to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image, ort_session):\n",
    "    input_name = ort_session.get_inputs()[0].name # get the input name of the model \n",
    "    output_name = ort_session.get_outputs()[0].name # get the output name of the model\n",
    "    result = ort_session.run([output_name], {input_name: image}) # run the inference on the image \n",
    "    probabilities = softmax(result[0][0]) # apply softmax to the output to get the probabilities\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. define output class names as in the pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['class_1', 'class_2', '...'] # class names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. run the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = preprocess_images('path/to/your/images') # preprocess the images\n",
    "for image in images:\n",
    "    probabilities = run_inference(image, ort_session) # run the inference on the image\n",
    "    class_idx = np.argmax(probabilities) # get the index of the class with the highest probability\n",
    "    class_name = class_names[class_idx] # get the class name\n",
    "    print(f'Class: {class_name}, Probability: {probabilities[class_idx]}') # print the class name and probability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
